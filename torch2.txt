TorchBench Benchmarking PyTorch with High API Surface Coverage - T3 Chat
New Thread
Press Enter to send, Shift + Enter for new line
Attach
Your message:

I'm reading the torch 2 paper(attached). Can you explain exactly how cuda graphs work, and the differences between graph capture at that level vs within the python bytecode that dynamo is doing?
torch2.pdf
Attach
Assistant Reply:

Here’s a concise, practitioner-level explanation that contrasts CUDA Graphs with PyTorch 2’s Python-bytecode graph capture (TorchDynamo), and how they fit together.

What CUDA Graphs are and how they work

    Goal: Cut host-side (CPU) overhead of launching many small kernels by recording a sequence of GPU work once and replaying it with near-zero per-iteration launch overhead.
    Capture:
        You begin a “stream capture” on a CUDA stream.
        Any subsequent GPU work launched on that stream (kernel launches, memcpys, events, dependencies) is recorded into a DAG (a cudaGraph).
        You end capture to obtain a cudaGraph, then instantiate it as a cudaGraphExec (validated, optimized replayable form).
    Replay:
        You invoke cudaGraphLaunch on the cudaGraphExec. The driver submits the entire recorded DAG in one go.
        This bypasses much of the CPU launch overhead and reduces CPU↔GPU synchronization points.
    Requirements and constraints:
        The recorded execution must be “graph-safe”: same structure each replay (same sequence of kernels/memcopies/dependencies), stable memory addresses for resources referenced by kernels, and compatible tensor/device states.
        Dynamic shapes/strides/allocations that change addresses can invalidate reuse unless you manage static allocations or use update APIs with limitations.
        It is a low-level mechanism: unaware of Python, autograd semantics, PyTorch ops, etc. It just records device work on a stream.

What TorchDynamo does (Python bytecode–level capture)

    Goal: Extract high-level computation graphs from imperative PyTorch code without asking users to rewrite it, enabling compiler optimizations (fusion, layout, scheduling) while preserving Python flexibility.
    How it works:
        Hooks CPython’s frame evaluation (PEP 523) to JIT-transform Python bytecode on the fly.
        Symbolically interprets Python bytecode, identifies and groups PyTorch ops into FX graphs, inserts guards for dynamic assumptions (tensor dtypes, shapes, control-flow choices), and compiles those graphs via a backend (default: TorchInductor).
        Supports partial capture: when it hits unsupported Python constructs, it emits a “graph break” and resumes later, interleaving compiled regions with normal Python execution.
    Scope:
        Operates above operators: understands Python control flow, function inlining, closures, side effects, mutation modeling; produces IR that can be optimized (fusion, reduction scheduling, memory planning) and then codegenned to Triton/C++.
        Integrates with AOTAutograd for training (splitting fwd/bwd graphs, decompositions, functionalization).
        Handles dynamic shapes by specializing with guards and meta functions; recompiles or switches variants when guards fail.

Key differences: CUDA Graphs vs TorchDynamo graph capture

    Layer of abstraction:
        CUDA Graphs: driver/runtime-level recording of already-lowered GPU work (kernels/memcopies). It does not change computation; it only packages launches efficiently.
        TorchDynamo: language/runtime-level program capture and transformation. It changes how computation is expressed (builds IR), enabling new fused kernels and schedules before kernels even exist.
    Purpose:
        CUDA Graphs: reduce CPU launch overhead and improve submission efficiency. No kernel fusion or algorithmic change.
        TorchDynamo (+Inductor): optimize the computation itself (fusion of pointwise/reduction/scatter, better memory traffic, templates for matmul/conv, loop/layout reordering), and then optionally use CUDA Graphs to cut remaining launch overhead.
    Sensitivity to dynamism:
        CUDA Graphs: require structural stability across replays. Dynamic shapes/allocations or control flow that changes the kernel DAG break reuse unless carefully managed. Often disabled when shapes/addresses vary.
        TorchDynamo: designed to tolerate Python dynamism via guards and graph breaks; can recompile variants as needed, and only enables CUDA Graphs when guardable/stable.
    Integration point:
        CUDA Graphs sit after codegen: they record the resulting sequence of kernel launches (from Inductor or eager).
        TorchDynamo sits before codegen: it decides what the kernels will be.
    Autograd/training:
        CUDA Graphs: agnostic. You can capture forward/backward launches if the sequence is stable and memory addresses don’t change, but correctness/structure is on you (or the framework) to ensure.
        TorchDynamo + AOTAutograd: explicitly produce forward/backward graphs, functionalize mutations, and ensure correctness around training semantics.

How PyTorch 2 combines them

    Typical pipeline with torch.compile(model, mode="reduce-overhead"):
        TorchDynamo captures Python into FX.
        AOTAutograd (if training) produces fwd/bwd graphs; decompositions simplify ops.
        TorchInductor lowers to a loop-level IR, fuses ops, and generates Triton/C++ kernels and lightweight wrappers.
        If analysis shows static-enough execution (no dynamic shapes/addresses that would vary), Inductor’s wrapper uses CUDA Graphs to record the kernel sequence and then replays it each iteration, removing residual launch overhead.
    Result: large speedups primarily from fusion/codegen; CUDA Graphs add further gains by shaving launch overhead when safe.

When to use what (mental model)

    If your model is small-kernel heavy with stable shapes and repetitive iteration, CUDA Graphs can yield notable gains even without compiler fusion (though fusion + graphs is best).
    If your model benefits from operator fusion, layout changes, or you have Python control flow and partial graphable regions, TorchDynamo/Inductor is essential.
    If shapes/addresses vary frequently, CUDA Graphs may be disabled; Dynamo/Inductor will still optimize and run, falling back to normal launches.

Common pitfalls and practical notes

    CUDA Graphs:
        Ensure allocations are “static” across iterations (e.g., use persistent buffers, avoid per-iteration new allocations that change pointers).
        Avoid host syncs or Python branches that force intervening CPU decisions based on yet-to-be-computed GPU results during capture/replay.
    TorchDynamo:
        Minimize data-dependent Python branching on tensor values to reduce graph breaks (or accept partial capture).
        Be aware that converting tensors to Python scalars (item(), tolist()) can force breaks or reduce capture scope.
        Dynamic shape specialization may trigger multiple compiled variants; that’s expected.

In one sentence:

    TorchDynamo captures and transforms your Python program into optimized kernels (changing what gets launched), while CUDA Graphs captures the final launches themselves (changing how they’re launched); PyTorch 2 uses both—first to make better kernels, then to launch them with minimal overhead when execution is stable.

GPT-5
58.19 tok/sec
1422 tokens
Time-to-First: 1.8 sec
Your message:

where are cuda graphs stored? are they stored as effectively large kernels, or is it something else?
Attach
Assistant Reply:

They’re not stored as a single “large kernel.” CUDA Graphs are stored as a driver/runtime-level data structure that represents a DAG of GPU work (kernels, memcpys, event waits, dependencies), plus an instantiated, validated form for fast replay.

What’s actually stored

    cudaGraph_t: an in-memory graph object (on the host side) containing nodes and edges:
        Kernel nodes: descriptors of individual kernel launches (function pointer, grid/block dims, shared mem size, kernel params).
        Memcpy/memset nodes: parameters for device↔device/host↔device copies, memset details.
        Event/wait nodes and stream dependencies.
    cudaGraphExec_t: an “executable” instantiation of the graph created by cudaGraphInstantiate. This is an optimized, validated version the driver can launch quickly. It may include internal scheduling metadata, resource bindings, and pre-validated launch parameters.

Where they live

    They’re owned by the CUDA runtime/driver in host memory, with references to device-side resources (pointers to device buffers, kernel entry points). They are not one compiled mega-kernel on the GPU.

How replay works

    On replay, cudaGraphLaunch submits the pre-validated DAG to the driver, which schedules the same sequence of original kernels/memcpys with recorded dependencies. You still execute many kernels; you just submit them as one object with minimal CPU overhead.

Implications

    Not a fused kernel: performance gains come from reduced CPU launch overhead and fewer sync points, not from combining kernels’ computations.
    Stability requirements: because node parameters include pointers, sizes, and launch configs, you typically need stable allocations and structure across replays. CUDA provides limited update APIs (e.g., to tweak kernel params) but structural changes require re-capture or re-instantiation.
    Memory footprint: you hold onto the cudaGraphExec_t object(s) for reuse. Frameworks like PyTorch keep these around in their wrappers when mode="reduce-overhead" is enabled and conditions are safe.

Quick mental model

    Think of a CUDA Graph as a pre-recorded playlist of kernel launches and copies with all knobs set, ready to “play” in one call—still multiple kernels, just packaged and validated—rather than a single giant kernel binary.

GPT-5
89.64 tok/sec
476 tokens
Time-to-First: 1.5 sec
Your message:

What does it mean for define by run semantics to be included in the IR? I thought the purpose of the graph capture was to automatically define and run on the eventually compiled graph
Attach
Assistant Reply:

Great question. The phrases “define-by-run” and “graph capture” sound contradictory at first, but they refer to different layers and purposes.

What “define-by-run” means in TorchInductor’s IR

    Define-by-run here describes how the compiler’s intermediate representation is specified, not how the model executes at runtime.
    TorchInductor’s loop-level IR uses executable Python functions/closures to define the computation of loop bodies (e.g., how to compute one element or a tile). Those functions call primitive ops (ops.load, ops.mul, ops.reduction, etc.).
    Because this IR is defined using real Python code, you can write lowerings concisely, re-use Python control flow, and compose kernels easily. Then the compiler traces and analyzes these Python functions to produce static code (Triton or C++).
    In other words: the IR is “defined by running” these small Python functions in a controlled, symbolic environment during compilation, not during model execution.

How this differs from model-level define-by-run

    Eager PyTorch is define-by-run at the model level: you run Python and it executes ops immediately.
    TorchDynamo captures that model-level execution into an FX graph, breaking the define-by-run behavior at the model level.
    TorchInductor then lowers that graph into its IR whose building blocks are Python functions that are executed symbolically to define loop bodies and memory access patterns. This is “define-by-run” within the compiler, not for end-user execution.

So do we still get a compiled, “define-and-run” graph?

    Yes. After TorchInductor constructs its IR using define-by-run functions, it:
        Analyzes and transforms the IR (fusion, scheduling, memory planning).
        Generates static kernels (Triton for GPU, C++/OpenMP for CPU) and lightweight wrappers.
        Optionally wraps the final launches in CUDA Graphs to remove launch overhead.
    At runtime, you are running compiled kernels with a static execution plan (subject to guarded specializations). There’s no Python-loop interpretation in the hot path.

Why TorchInductor chose a define-by-run IR

    Ergonomics: Lowerings are simpler to write and maintain in Python; fewer bespoke IR node types are needed.
    Power/flexibility: You can encode complex index math and control flow in normal Python, then trace it to a graph for analysis/codegen.
    Performance: Despite the IR being defined by running Python functions, that “running” happens at compile time (symbolic/tracing mode). The emitted kernels are fully compiled and static for runtime.

Mental model

    Stage 1 (TorchDynamo): capture your imperative, model-level define-by-run program into a high-level graph.
    Stage 2 (TorchInductor): use a define-by-run style to specify loop bodies and kernel structure during compilation; analyze that to produce static kernels.
    Stage 3 (Runtime): execute the compiled kernels (possibly via CUDA Graphs). No Python interpretation in the hot path.

Bottom line

    “Define-by-run IR” refers to how the compiler authors define and manipulate the IR during compilation (using executable Python to specify semantics), not to how your model runs. The end result is still a compiled, optimized “define-and-run” execution of your model.

GPT-5
44.83 tok/sec
683 tokens
Time-to-First: 2.3 sec
Your message:

how much python code exists in the output model from torch compile? if there are no graph breaks, can it go down to zero?
Attach
Assistant Reply:

Short answer

    Yes, if there are no graph breaks and the compiled region is “graph-safe,” the hot path can run with effectively zero Python in the iteration loop.
    In that best case, PyTorch 2 (TorchDynamo + AOTAutograd + TorchInductor) emits:
        Compiled kernels (Triton on GPU, C++/OpenMP on CPU)
        A lightweight wrapper to set up sizes/strides, buffers, and launch kernels
        Optionally a CUDA GraphExec to replay the whole launch DAG
    With CUDA Graphs enabled (e.g., mode="reduce-overhead") and shapes/addresses stable, the wrapper’s per-iteration work is minimal and avoids Python overhead; replay is a single driver call.

More detail

    No graph breaks means the Python-level model code for that region has been replaced by:
        Generated kernel(s) and a generated wrapper function. There are two wrapper flavors:
            Python wrapper: default, simple but has some Python call overhead
            C++ wrapper: lower overhead path (Inductor can emit a compiled C++ entry point)
        If safety checks pass (static-enough shapes/addresses), the wrapper will capture once into a cudaGraphExec and then replay it every iteration, which removes even wrapper-side launch overhead from the steady state.
    What Python remains?
        Outside the compiled region: any user-driven loop, dataloader, logging, optimizer steps (unless also compiled) still run in Python.
        Inside the compiled region: none of the original Python executes. The only Python that may remain is:
            The Inductor “driver”/wrapper function if you use the Python wrapper variant.
            Guard checks on entry to ensure assumptions (shapes/dtypes/layout, etc.) still hold. If you use the C++ wrapper and/or CUDA Graph replay, these checks are minimal and not per-op.
    Training case:
        AOTAutograd splits forward/backward and Inductor compiles both. If both halves are captured without breaks and are graph-safe, the training step can similarly run with near-zero Python in the hot path, aside from the optimizer unless you also compile or fuse it.

Typical configurations

    mode="default": Python wrapper is used; overhead is already low because most work is in compiled kernels.
    mode="reduce-overhead": tries to use CUDA Graphs to eliminate wrapper launch overhead when safe; steady-state Python in the hot path becomes negligible.
    mode="max-autotune": same as above but with extra autotuning during compile; runtime path remains the same as whatever wrapper/graphs are selected.

Practical takeaway

    You don’t need absolutely zero Python to get most of the speedup: kernel fusion and codegen dominate. But if your model is stable-shaped and has no graph breaks, PyTorch 2 can reduce Python in the steady-state execution path to near zero, especially with CUDA Graphs or a C++ wrapper.

GPT-5
60.65 tok/sec
617 tokens
Time-to-First: 1.8 sec
Your message:

can you summarize the handling of dynamic shapes in torch 2?
Assistant Reply:

Here’s a compact summary of how PyTorch 2 (torch.compile with TorchDynamo + AOTAutograd + TorchInductor) handles dynamic shapes.

Core idea

    Specialize with guards instead of requiring fully static shapes. The compiler captures one executable variant under assumptions about sizes/strides/layout/etc., records those assumptions as guards, and reuses the compiled artifact as long as guards hold. If guards fail, it recompiles or selects another cached variant.

Key pieces

    Symbolic sizes: Input tensor sizes become SymPy symbols in the compiler. Those symbols flow through meta functions that compute output shapes without doing real compute.
    Meta functions: For each operator, a “meta” implementation predicts output sizes/strides given input symbols. Coverage is broad (thousands of ops); gaps can trigger graph breaks.
    Guarding:
        Size/stride/layout/dtype/modules/global-state constraints get recorded as guards during capture.
        On subsequent runs, guards are checked quickly. If they pass, reuse; if not, compile a new variant.
        Many guard simplifications are applied (e.g., dedup, SymPy-based implication checks).
    0/1 specialization:
        Dimensions equal to 0 or 1 are aggressively treated as constants with guards, because they strongly affect contiguity/broadcast logic and simplify reasoning.
        If a dimension is symbolic, the system assumes it cannot be 0/1 (useful negative information).
    Handling control flow:
        The compiler specializes on the branch taken during the initial run (“size hint” driven). It avoids building multi-branch symbolic graphs; instead it uses guards to ensure the same branch remains valid. If not, a new variant is produced.
    Unbacked symbolic integers:
        Sizes that depend on data (e.g., from nonzero(), item()) cannot be known during capture. Control flow depending on such values forces a graph break. Several mitigations reduce unnecessary breaks by avoiding or lazily computing certain precomputed tensor properties.
    AOTAutograd:
        For training, forwards is captured; autograd is run on fake tensors to record a joint fwd+bwd graph; then a min-cut split yields separate optimized fwd/bwd graphs. This respects dynamic-shape specialization and uses decompositions/functionalization.
    TorchInductor codegen:
        Generates kernels (Triton/C++) parameterized by symbolic extents, with loop bounds and index math expressed in terms of the shape symbols.
        Scheduler and fusion operate in the presence of symbolic sizes (e.g., legality checks use symbolic address expressions).
    Variant caching:
        Multiple compiled variants for the same function may exist, keyed by different guard sets (e.g., different batch sizes or shape relations).
        Fast guard checks choose the appropriate variant at runtime.

When CUDA Graphs are used

    CUDA Graphs require structurally identical launch DAG and stable addresses across replays. Inductor enables them only when dynamic shapes/allocations won’t change those invariants. If shapes vary, CUDA Graphs are typically disabled for that region, but Inductor’s fused kernels still run.

What’s not targeted

    Dynamic rank (changing number of dimensions) is intentionally not supported—it’s rare in real DL workloads and would complicate symbolic reasoning substantially.

Practical implications

    You can pass varying batch sizes or sequence lengths; the system compiles and reuses variants as long as guards allow.
    Data-dependent shapes inside kernels can cause graph breaks; keeping such logic at the edges or using padding/bucketing reduces breaks.
    Most performance wins come from fusion/codegen that are shape-symbolic; you don’t need fully static shapes to benefit.

GPT-5
46.74 tok/sec
760 tokens
Time-to-First: 3.1 sec
Your message:

does this paper have any speed comparisons with XLA
Assistant Reply:

Yes. The paper includes head-to-head results with PyTorch/XLA as one of the backends.

    Where: Table 3 (“Geometric mean speedups over PyTorch eager”) and Figure 4 (CDFs) compare TorchInductor against several TorchDynamo backends, including PyTorch/XLA.
    What they report (high level):
        GPU (A100), float32 inference: PyTorch/XLA geomean ≈ 0.80× on TorchBench (slowdown), ≈1.03× on HuggingFace (about break-even), ≈1.24× on TIMM (speedup).
        GPU (A100), float16 inference: ≈0.82× on TorchBench, ≈1.16× on HuggingFace, ≈1.59× on TIMM.
        GPU training: generally lower than TorchInductor; some configs show slowdowns (e.g., 0.73× on TorchBench float32 training).
        CPU: XLA shows substantial slowdowns in their setup (e.g., 0.27× on TorchBench CPU inference).
    Context: All backends share the same front end (TorchDynamo) and graphs; only the backend compiler differs. In these comparisons, TorchInductor has the best geomean in most settings.

GPT-5
17.80 tok/sec
283 tokens
Time-to-First: 2.7 sec
1 tool call
Your message:

how is symbolic execution used in this paper?
Assistant Reply:

Symbolic execution appears in two main layers of the PyTorch 2 stack described in the paper: TorchDynamo (Python-bytecode capture) and TorchInductor (lowering/IR/codegen). In both, “symbolic” means they run an interpreter that executes instructions over abstract values (symbols/trackers) instead of real tensors, to extract graphs, guards, and IR without doing real numeric compute.

    TorchDynamo: symbolic evaluation of Python bytecode

    Purpose: Extract FX graphs from eager Python while keeping Python’s flexibility; decide where to insert graph breaks; record guards on dynamic assumptions.
    How it works:
        Frame hook replaces CPython’s eval loop and symbolically interprets bytecode one instruction at a time.
        Uses VariableTracker objects (TensorVariable, ListVariable, ConstDictVariable, UserFunctionVariable, etc.) to represent stack slots, locals, and heap objects symbolically.
        When it encounters tensor ops, it emits FX nodes using a Proxy; no real tensor math occurs. It also accumulates guards (types, dtypes, sizes/strides, module attrs, global state).
        It symbolically inlines user functions/closures where profitable (with checkpoint/rollback if inlining fails), and specializes away many control-flow constructs by guarding on the conditions (e.g., tensor metadata based branches).
        For side effects (mutations to globals/attrs/lists/cells), it records them in a side-effect log and emits equivalent mutations in generated output bytecode after the compiled graph call.
        If it sees something unsupported (e.g., data-dependent branch on a tensor value), it emits a graph break and generates continuation functions to resume analysis mid-function.

    Dynamic shapes: symbolic sizes and guards

    Purpose: Support shape variability without requiring fully static shapes.
    How it works:
        Sizes/strides are modeled as SymPy symbols. Meta functions (per-op “shape evaluators”) propagate these symbols to compute output shapes/strides without real compute.
        Branches on shapes are specialized to the observed branch and guarded (“size hints”); no multi-branch symbolic graphs are built.
        Aggressive 0/1 specialization simplifies contiguity/broadcast logic; symbolic dims are known to be not 0/1 unless guarded otherwise.
        Unbacked symbolic integers (from data-dependent ops like nonzero().size(0) or tensor.item()) cannot be resolved; control flow depending on them forces graph breaks. The paper details mitigations to reduce unnecessary guards/breaks (lazy precomputed properties, constraints).

    TorchInductor: symbolic interpretation to lower FX to define-by-run loop IR

    Purpose: Convert an FX graph into a loop-level IR using Python functions that describe element/tile computations, then analyze/transform it.
    How it works:
        Symbolically interprets FX nodes and applies operator “lowerings” that construct TensorBox/StorageBox/ComputedBuffer objects and inner_fn closures that compute single elements via ops.* primitives (ops.load/store/reduction/index_expr/etc.).
        Those inner_fn closures are executable Python, but they are executed/traced in a controlled environment where ops.* is a virtualized namespace. By swapping ops.* implementations, Inductor can:
            Analyze: record memory accesses, symbolic addresses, high/low watermarks.
            Transform: FX-trace the closures to build a graph of primitive ops.
            Codegen: print Triton/C++ code, doing CSE and index simplifications.
        Scheduling/fusion operate on symbolic memory addresses and loop ranges (SymPy expressions), checking legality (e.g., read-after-write hazards, reversed traversal) symbolically.

    Guards: symbolic assumptions checked at runtime

    Purpose: Reuse compiled artifacts safely.
    Collected during Dynamo’s symbolic pass (and also by AOTAutograd/Inductor) and baked into a guard function per transformed code object.
    On entry, guards are evaluated quickly; pass → reuse; fail → compile/select another variant.

    AOTAutograd with fake tensors (symbolic gradients)

    Purpose: For training, produce forward/backward graphs compatible with partial program capture.
    How it works:
        Runs autograd on “fake” tensors (metadata-only) to record a joint graph; splits via a min-cut into forward/backward; applies decompositions and functionalization. This relies on symbolic/meta execution since there is no real data.

In short:

    TorchDynamo uses symbolic execution of Python bytecode to extract FX graphs, side-effect plans, and guards.
    For dynamic shapes, sizes/strides are symbolic (SymPy), and control flow is specialized with guards.
    TorchInductor uses symbolic interpretation of FX to build a define-by-run loop IR, then swaps in analyzers/codegen backends that operate on symbolic index/math to fuse, schedule, and emit Triton/C++.
    AOTAutograd uses symbolic/fake tensors to derive training graphs.

GPT-5
101.07 tok/sec
1030 tokens
Time-to-First: 1.3 sec

