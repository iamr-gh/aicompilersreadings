T3 Chat
New Thread
Press Enter to send, Shift + Enter for new line
Attach
Your message:

what is the meaning of beam search in the context of this paper. How different is that from some kind of genetic algorithm with a pareto frontier?
ansor.pdf
Attach
Assistant Reply:

Beam search in this paper refers to the “sequential construction based search” strategy (as in the Halide auto-scheduler) where the compiler builds a program by making a fixed sequence of decisions (e.g., tiling, compute/ storage locations) and keeps only the top-k partial candidates after each decision according to a learned cost model. Key properties:

    It operates on incomplete programs: at each step, candidates are partially specified.
    Early pruning: only the best k partial candidates (the “beam”) are retained; others are discarded.
    Fixed decision order: the construction follows a predetermined sequence of choices.
    Efficiency comes from aggressive narrowing at each step, but it risks discarding partial candidates that would have led to the best final program because the model is inaccurate on incomplete programs.

How that differs from a genetic/evolutionary algorithm with Pareto frontier (multi-objective GA):

    Candidate representation and evaluation

    Beam search: candidates are partial (until the end), and the model must score incomplete programs to prune. This paper shows such scoring is inaccurate early, leading to suboptimal pruning.
    Evolutionary (as in Ansor): works on complete programs sampled from a large space; a learned cost model scores complete programs (much more accurate), and mutation/crossover refine them.

    Search dynamics

    Beam search: deterministic, layer-by-layer expansion with top-k pruning at each step; limited ability to “recover” from a bad early pruning decision.
    Evolutionary: stochastic, population-based; mutations and crossovers can make out-of-order changes and combine good “genes” from different parents, enabling recovery and broader exploration.

    Space coverage

    Beam search: constrained by the fixed unfolding order and early pruning; harder to include transformations that add nodes or change structure (e.g., cache stages, rfactor) when they disrupt the fixed decision sequence.
    Evolutionary: can handle richer, hierarchical spaces; Ansor samples diverse high-level sketches first, then tunes low-level details via evolution.

    Objective handling

    Beam search: typically single scalar score per candidate (from a cost model), so pruning is by that scalar.
    Pareto-based GA: maintains a set of non-dominated solutions across multiple objectives (e.g., latency, memory, code size); selection favors Pareto-front individuals, preserving trade-offs.

    When multi-objective matters

    This paper primarily optimizes throughput/latency as a single objective. Beam search fits single-objective pruning. A Pareto GA would be preferable if you jointly optimize, say, runtime, memory footprint, and compilation time, maintaining multiple trade-offs instead of collapsing them into one score.

Bottom line:

    Beam search = top-k pruning of partial programs along a fixed decision path; fast but vulnerable to inaccurate early estimates and limited space.
    Genetic/evolutionary with a Pareto frontier = population-based optimization of complete programs with stochastic variation, preserving multiple trade-offs; more exploratory and robust to early misestimation, and better suited for multi-objective tuning. Ansor’s approach is closer to evolutionary search (single main objective), augmented by a hierarchical sampling strategy and a learned cost model to evaluate complete programs.

GPT-5
80.22 tok/sec
651 tokens
Time-to-First: 1.7 sec
Your message:

can you explain what this paper means by a hierarchical sampling space and why that is significant, using some tangible examples? I am not grasping how this differs from other search approaches
Attach
Assistant Reply:

Short answer first: Hierarchical sampling means Ansor splits the search space into two layers:

    High-level “sketches” = the structural shape of a program (which loops exist, how they’re tiled, what gets fused or cached, whether reductions are factorized, where things compute).
    Low-level “annotations” = the concrete, detailed choices that fill in a sketch (exact tile sizes, which loops to parallelize/vectorize/unroll, pragma values, compute-at positions among valid attach points, etc.).

Ansor first enumerates a small set of diverse high-level sketches, then repeatedly samples and fine-tunes many low-level annotations per sketch. This is significant because it:

    Explores fundamentally different program structures (that templates or sequential/beam methods often miss).
    Evaluates only complete programs (avoiding inaccurate scoring of partial programs).
    Separates combinatorially huge detail choices from the far smaller space of structural alternatives, so it can cover more meaningful designs efficiently.

Tangible examples

Example 1: Matmul + ReLU (C = A·B, D = ReLU(C))

    Sketch level (structure):
        Decide a tiling pattern for matmul loops (on CPU they use an SSRSRS pattern: multiple levels of tiling on space loops i,j and reduction loop k).
        Decide whether to fuse the ReLU into the matmul’s inner loops or leave it as a separate stage.
        Decide whether to insert a cache write stage for the output to improve write-combining (if there’s no consumer to fuse).
        These structural choices produce a handful of distinct sketches (often under 10 per subgraph).

    Annotation level (details):
        For a chosen SSRSRS structure, pick exact tile sizes for each tiling level (e.g., i0=64, j0=64, k0=256; i1=8, j1=8, k1=8; etc.).
        Choose which outer loops to parallelize, which inner loops to vectorize, and unroll factors.
        Set pragmas like auto_unroll_max_step.
        Optionally tweak compute-at locations for small flexible nodes.
        Each sketch admits billions of valid detail combinations; Ansor randomly samples many, then evolves them.

Why this matters: Two schedules with different fusion/caching structures can behave radically differently on modern CPUs/GPUs, even before you tune sizes. Traditional template-guided methods often hardcode one or two structures (e.g., specific tiling and fixed fusion plan), then tune parameters. Beam/sequential methods can struggle to introduce structural changes (like adding cache nodes or doing rfactor) because they rely on a fixed order of decisions and must score partial programs accurately—which is hard. Ansor, by lifting these structural choices into few explicit sketches, guarantees they are all explored and only compares complete programs.

Example 2: Attention subgraph (Transpose → BatchMatMul → Softmax)

    Sketch level:
        Decide the fusion boundary (e.g., fuse one transpose into the matmul producer-consumer chain vs keep separate).
        Choose a multi-level tiling that aligns with memory reuse across heads and sequence length.
        For GPU, decide whether to insert shared-memory caches and cross-thread reductions.

    Annotation level:
        Bind specific loop levels to CUDA blockIdx/virtual threads/threadIdx.
        Choose tile sizes per level, vector widths, unroll steps, shared memory staging sizes.
        Select cross-thread reduction patterns and warp shuffles.

Why this matters: On GPU, the structural choice of which tile levels map to blocks/threads and whether to introduce shared-memory staging dominates performance. Templates typically encode one mapping. Beam search would need to commit to thread/block mappings early and may prune promising lines due to cost-model error on partial schedules. Ansor samples multiple viable mappings as distinct sketches, then tunes details within each.

How this differs from other search approaches

    Versus template-guided search (e.g., AutoTVM/FlexTensor):
        Templates predetermine both structure and which parameters are tunable. You can only tune inside that structure. If the best structure isn’t in the template (e.g., extra cache stage, different multi-level tiling depth, rfactor), you’ll never find it.
        Ansor auto-generates sketches by rules (multi-level tiling, fuse consumer, add cache, rfactor, GPU shared-memory, etc.) and so covers more structures without hand-writing templates.

    Versus sequential/beam search (e.g., Halide auto-scheduler):
        Beam search expands decisions in a fixed order and keeps top-k partial schedules via a cost model. The paper shows models trained on complete programs predict poorly on partial ones, so early pruning can drop paths that would have led to the best final program.
        Ansor never needs to rank partials: it samples a complete program (sketch + annotations), evaluates with a model that is accurate on complete programs, and then refines via evolutionary mutations/crossovers.

    Versus a flat evolutionary search over everything:
        If you try to mutate everything at once from the naive program, the space is astronomically large and mixing structural and parametric edits can be inefficient.
        Ansor’s hierarchy constrains structure first (few sketches), then lets evolution work within each structure’s massive parameter space. That balances breadth (different structures) and depth (fine-tuning details).

A quick mental model

    Think of “sketches” like choosing a floor plan for a house: how many rooms, which rooms connect, where to place the staircase.
    “Annotations” are interior design choices: exact dimensions within code constraints, paint colors, fixtures.
    Template-guided search gives you one or two fixed floor plans and lets you pick colors. Beam search tries to sketch the floor plan step-by-step but throws away partial drawings based on a fuzzy preview.
    Ansor enumerates several plausible floor plans, then rapidly tries many interior choices for each—and iteratively improves them—judging only finished houses.

Practical consequences

    Wider coverage: finds schedules outside existing templates (e.g., T2D, NRM parallel reductions).
    More reliable optimization: avoids the pitfall of scoring partials.
    Portability: the same derivation rules produce sketches for CPUs and GPUs (with GPU-specific rules), keeping the approach general.
    Efficiency: only a handful of sketches per subgraph, but billions of variants within—sampling plus evolution explores that efficiently.

If helpful, I can walk through a specific operator you care about and sketch what its sketches and annotations would look like on CPU vs GPU.
GPT-5
69.78 tok/sec
1366 tokens
Time-to-First: 1.9 sec
Your message:

can you explain how the execution cost model works? how does it offer meaningful simplification than running the program itself
Attach
Assistant Reply:

Ansor’s execution cost model is a learned predictor that estimates how fast a complete tensor program will run, without actually compiling and timing it. It replaces expensive “compile-and-measure” loops for most candidates with a fast scoring pass, and only a small number of top-scoring programs are actually measured to provide labels for retraining.

What the model predicts

    Target: throughput (higher is better) of a complete program.
    Granularity: it scores each innermost non-loop statement in the program’s loop nests, then sums those scores to get a program-level score. This keeps the model focused on the hotspots where work and memory traffic happen.

Why not score partial programs?

    The paper shows models trained on complete programs are inaccurate on partial schedules (poor pairwise accuracy and top-k recall early in construction). So Ansor samples complete programs first (sketch + annotations), then uses the model where it’s accurate.

What inputs (features) the model uses
Ansor extracts a structured feature vector that captures the key performance determinants for the innermost statement, in the context of its surrounding loop nest:

    Arithmetic features: counts of float/int ops, intrinsic math calls, etc.
    Loop transformation features:
        Vectorization: innermost vector width, position (which loop), number of vectorized loops.
        Unrolling: unroll factors and positions.
        Parallelization: which outer loops are parallelized and their sizes.
        On GPU: blockIdx/threadIdx sizes and virtual threads.
    Memory-access features per buffer used by the statement (for up to 5 buffers):
        Bytes and unique bytes accessed; cache lines and unique lines.
        Reuse type (e.g., loop-carried, serial), reuse distance, reuse count.
        Strides; bytes-per-reuse metrics (bytes/reuse_count, lines/reuse_count, etc.).
    Arithmetic intensity curve: the ratio FLOPs/bytes at each loop level (10 sampled points).
    Allocation/context features: outer-loop depth, product of outer loop extents, pragma values such as auto_unroll_max_step.
    These features are designed to approximate locality, bandwidth pressure, vector utilization, parallel granularity, and compute/memory balance—i.e., the major factors driving runtime.

Model form and training

    Model: gradient boosted decision trees (XGBoost-style).
    Label generation: a subset of candidate programs are compiled and timed on the real hardware; their measured throughputs provide labels.
    Loss: weighted squared error, with higher weight for faster programs so the model prioritizes ranking the top end correctly:
    loss(f, P, y) = y · (sum_s f(s) − y)^2
    Normalization: throughputs per DAG are normalized to [0, 1] to make the model shareable across many operators.
    Retraining: fast (tens of thousands of samples at most), so the model is retrained frequently as new measurements arrive.

Why this is a meaningful simplification versus running programs

    Orders-of-magnitude faster evaluation:
        Running a candidate requires lowering, compiling, launching, warmups, repetitions, timers, and sometimes cache control—milliseconds to seconds per candidate.
        Model inference on feature vectors is microseconds to sub-milliseconds per candidate. This lets Ansor score tens or hundreds of thousands of candidates in seconds and only measure the most promising few.
    Good enough fidelity where it matters:
        On complete programs, the model achieves high correlation (reported R^2 ≈ 0.958, pairwise accuracy ≈ 0.851, top-30 recall ≈ 0.624 in a test set from ResNet-50 tuning), which is sufficient to rank candidates and focus measurements.
        It is much less reliable on partial programs; Ansor avoids that use case by construction.
    Captures dominant effects without simulating everything:
        The features proxy key bottlenecks (e.g., reuse distance for cache behavior, arithmetic intensity for compute vs memory bound, vector width and alignment, parallel chunk sizes).
        Decision trees can learn nonlinear thresholds (e.g., when a tile fits in L1/L2/shared memory) without an explicit architectural simulator.
    Hardware- and workload-adaptive:
        It learns directly from measurements on the target hardware and operator set, so it adapts to microarchitectural quirks and compiler codegen behavior that are hard to model analytically.

How it’s used in the search loop

    Sample many complete programs (sketch + annotations).
    Use the cost model to score them quickly and pick a small batch.
    Measure only that batch on real hardware; add results to the training set.
    Retrain the model; run evolutionary mutations/crossovers guided by model scores.
    Repeat, gradually improving both the model and best-found programs.

In short: the cost model is a fast, learned proxy for runtime that focuses on complete programs and the performance-critical inner statements, using features that reflect compute intensity, locality, and parallel/vector usage. It massively cuts down expensive measurements while still steering the search toward genuinely fast implementations.
GPT-5
14.34 tok/sec
1051 tokens
Time-to-First: 2.9 sec
Your message:

Even thought ansor was implemented into TVM, it seems like Halide got more industry use(despite this paper claiming ansor was better), why is that?
Attach
Assistant Reply:

A few practical factors explain why Halide has seen broader industry use even though Ansor (in TVM) often outperforms prior auto-schedulers in research benchmarks:

    Problem domain fit

    Halide’s sweet spot is image/video pipelines (stencil-like, regular, dense), which are ubiquitous in cameras, mobile imaging, video processing, AR, etc. That single, very large vertical (camera/imaging stacks across Android/iOS devices and many ISPs) created strong product pull.
    TVM/Ansor targets general deep learning operators and end-to-end NN inference. While that market is huge, many production teams rely on vendor libraries (cuDNN, oneDNN/MKL-DNN), ONNX Runtime, TensorRT, CoreML, XLA, etc., which already deliver solid performance and are tightly integrated with frameworks and hardware vendor tooling.

    Maturity and timeline

    Halide has been around since 2012–2013 and accumulated a long track record, stability, and production hardening, with a strong manual scheduling culture and later auto-scheduler options.
    TVM is newer (open-sourced 2017–2018), and Ansor arrived later (2020+). It takes time for new auto-scheduling tech to propagate, be stabilized across backends, and earn production trust.

    Control vs automation

    Halide lets expert engineers hand-craft schedules with very precise control, which big imaging teams value for predictability, debuggability, and guarantees across edge cases. Many production users still prefer curated schedules/templates to fully automatic search when SLAs and tight footprints matter.
    Ansor emphasizes automated search over a very large space. That’s powerful, but in production some teams prefer predictable compile time, deterministic behavior, and bounded variability.

    Integration and dependency footprint

    Halide integrates well as a library for image processing stages, with minimal runtime footprint. It’s relatively easy to drop into C++ codebases on mobile and embedded.
    TVM/Ansor shines for compiling neural network graphs and custom ops, but integrating a full NN compiler stack may be heavier than calling vendor NN runtimes, especially on tightly constrained platforms.

    Vendor ecosystems and hardware support

    Imaging pipelines have long-standing paths into ISPs, GPUs, and CPUs with Halide front-ends or Halide-inspired tooling, sometimes backed by hardware vendors.
    In DL inference, hardware vendors push their own stacks (TensorRT for NVIDIA, XLA/MLIR-based for TPU/accelerators, oneDNN for Intel, NNAPI/CoreML for mobile). Those stacks can reduce the need for third-party auto-scheduling, narrowing TVM/Ansor’s adoption to cases where portability or custom ops are key.

    Operational considerations

    Auto-tuning cost: Ansor needs a tuning phase (hours can be acceptable pre-deployment, but it’s still operational overhead) and per-shape tuning if shapes change. Many production settings favor plug-and-play kernels with zero tuning.
    Determinism and reproducibility: manual/templated schedules are easier to lock and certify; auto-tuned results depend on search seeds, cost-model training data, and measurement noise unless carefully managed.

    Community and incumbency

    Halide built a strong community in graphics/vision; many companies hired teams experienced with its scheduling language.
    TVM has strong traction too (esp. in cloud, research, and startups), but the DL production ecosystem is fragmented and often “good enough” with vendor solutions, slowing broad, default adoption.

How they’re used today

    Halide: widely used in camera/imaging/video stacks, photo processing apps, and vision libraries; sometimes also for DSP-like workloads beyond imaging.
    TVM/Ansor: used where portability across accelerators matters, for custom operators, for research/production needing unified compilation across CPU/GPU/NPU, or when vendor kernels underperform on unusual shapes/ops. It’s also embedded in other systems and research compilers.

Bottom line
Ansor showed compelling technical advantages in auto-scheduling tensor programs, but widespread industry adoption hinges on domain fit, existing vendor stacks, need for manual control, integration costs, and the time needed to build production trust. Halide’s early focus and dominance in a massive imaging vertical, plus its manual scheduling ergonomics, made it the default there—even as Ansor/TVM pushes state-of-the-art automation for deep learning workloads.
GPT-5
66.70 tok/sec
912 tokens
Time-to-First: 2.6 sec
Your message:

are there any other autoschedulers that have been shipped in production?
Attach
Assistant Reply:

Yes—several autoschedulers and ML-guided optimizers have shipped in production across different domains. A non-exhaustive but representative list:

Imaging and DSP-style pipelines

    Halide auto-schedulers
        Mullapudi et al. 2016 autoscheduler: shipped in Halide and widely used in production image/video pipelines (mobile camera stacks, AR, photo apps). Generates CPU/GPU schedules without tuning, within seconds.
        Adams et al. 2019 (beam-search + learned cost model): integrated into Halide; used by industry teams that prefer automated starting schedules which can be edited or accepted as-is.

Deep learning compilers/runtimes

    Apache TVM auto-scheduler (Ansor)
        Integrated into TVM as tvm.auto_scheduler. Used in production by some cloud providers, startups, and teams needing portability or custom ops. It does require a tuning phase, so adoption tends to be where that tradeoff makes sense.
    NVIDIA TensorRT/TensorRT-LLM autotuning
        TensorRT performs tactic selection/auto-tuning for kernels (e.g., GEMM/conv algorithms), and TensorRT-LLM provides build-time “auto” plugin selections and scheduling features. Deployed widely in production inference.
    XLA/MLIR-based autoscheduling/auto-optimization
        Google MLGO (LLVM-integrated RL policies for inlining and register allocation) has been deployed in production compilers used across large codebases and services; while not an “autoscheduler” for tensor loops per se, it’s a shipped ML-guided optimizer.
        IREE/MLIR pipelines include automatic tiling, vectorization, and codegen heuristics; some deployments exist in products and OSS stacks (often with heuristic scheduling rather than long per-model tuning).
    oneDNN (MKL-DNN) and cuDNN
        Both include internal autotuning/heuristics to choose algorithms and blockings at runtime or build time; these are widely used in production via PyTorch, TensorFlow, ONNX Runtime, etc.

GPU DSLs and operator generators

    Triton
        Provides an autotuner for kernel parameters (e.g., tile sizes, num warps). Used in production by multiple companies for custom GPU ops (notably in LLM inference/training stacks).
    CUTLASS
        Has profiler tooling that selects best kernel configurations per shape/architecture; used to generate kernels embedded in downstream libraries.

Training frameworks’ autotuners (not loop schedulers, but shipped and used)

    DeepSpeed Autotuning
        Production feature that searches ZeRO stage, micro-batch size, and related knobs to maximize throughput.
    NVIDIA NeMo Auto Configurator
        Production tool that searches model/parallelism hyperparameters (TP/PP/CP/EP, batch sizes, etc.) to maximize training throughput.

Polyhedral compilers

    Tiramisu, Tensor Comprehensions
        Shipped academically/OSS; real production adoption is more limited than Halide/TVM, but some teams integrate parts of their scheduling/auto-optimization flows.

How they differ in practice

    No-/low-tuning autoschedulers (Halide) are favored when you need quick, deterministic schedules for image pipelines with predictable access patterns.
    Search-based DL autoschedulers (TVM/Ansor) are favored when per-model tuning time is acceptable in exchange for higher peak performance or portability across heterogeneous hardware.
    Vendor runtimes (TensorRT, cuDNN, oneDNN) ship with built-in autotuners/heuristics and are the default in most production DL serving, especially when the operator set is standard and hardware-specific stacks are acceptable.
    Compiler-integrated ML optimizers (MLGO) are already in production toolchains, improving general-purpose codegen decisions.

If you share your workload type (imaging vs DL inference vs training), hardware targets, and whether you can afford per-model tuning time, I can recommend which production autoscheduler/auto-tuning stack tends to work best.
GPT-5
25.49 tok/sec
837 tokens
Time-to-First: 0.89 sec
1 tool call
