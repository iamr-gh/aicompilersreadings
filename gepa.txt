New Thread
Press Enter to send, Shift + Enter for new line
Attach
Your message:

What is a pareto frontier, contextualized by its use in this paper?
gepa.pdf
Attach
Assistant Reply:

In this paper’s context, the Pareto frontier is the set of “best so far” prompt candidates that are non-dominated across individual training instances.

    What “non-dominated” means here: For each training example, GEPA tracks which candidate achieved the highest score on that example. Any candidate that is the top performer on at least one example is kept; if another candidate is at least as good on every example and strictly better on one, it dominates the first and the dominated one is removed.
    Why this matters for GEPA: Instead of always mutating the single global-best prompt (which risks getting stuck in a local optimum), GEPA samples its next parent from the Pareto frontier. This preserves diverse, “winning” strategies discovered on different examples (e.g., one prompt excels on instance A, another on instance B), and allocates optimization steps to them stochastically, weighted by how often they appear on the frontier.
    Practical effect: Maintaining and sampling from the Pareto frontier balances exploration and exploitation, improves sample efficiency, and helps GEPA steadily combine complementary lessons from multiple strong candidates, rather than overfitting to one trajectory.

GPT-5
48.15 tok/sec
241 tokens
Time-to-First: 4.1 sec
Your message:

Where exactly in the prompt compilation stack does this method lie? I thought fine tuning involved changing weights, but it seems like this works with blackbox models and just doing RL on the generation of the prompt itself
Attach
Assistant Reply:

Great question. Think of the “prompt compilation stack” as having two broad layers you can optimize:

    Weight space (model internals): finetuning, LoRA, full RL on parameters. Requires gradient access and white-box or at least train-time APIs. Changes the model’s behavior globally.
    Program/prompt space (model externals): editing system prompts, few-shots, module wiring, tool calls, control flow, decoding settings. Works with black-box models because you only change inputs and orchestration.

Where GEPA sits

    GEPA operates entirely in the program/prompt space. It treats a compound AI system as a set of LLM modules (each with its own system prompt and possibly few-shots) plus control flow and tools.
    GEPA evolves the instructions (prompts) for those modules using reflective natural-language feedback and a Pareto-frontier search over training instances.
    No model weights are changed; GEPA is compatible with black-box models and paid APIs.

How this differs from RL on weights (e.g., GRPO)

    GRPO (and similar RLVR methods) updates parameters (often via LoRA or full FT), using scalar rewards from rollouts. That needs training access and large rollout budgets.
    GEPA “learns” in language: it analyses execution/evaluation traces, proposes new instructions, validates them, and keeps a diverse pool of non-dominated prompts (the Pareto front). It’s closer to Auto Prompt/Program Optimization than to policy-gradient training.

Relationship to other prompt-level methods

    Compared to Bayesian prompt search (e.g., MIPROv2), GEPA:
        Uses rich textual feedback (not only scalar scores) to propose targeted instruction edits.
        Maintains a Pareto frontier across individual instances to avoid local optima and preserve diverse strategies.
        Can also do “system-aware merge” (a crossover of module prompts) to combine learned sub-skills.

Takeaway

    You’re right: this is optimization at the prompt/program layer, not weight-space finetuning. It’s designed to work with black-box LLMs, gaining many of the benefits of “training” (task adaptation, sample efficiency) without touching weights—and often with far fewer rollouts than RL on parameters.

GPT-5
67.85 tok/sec
469 tokens
Time-to-First: 3.5 sec
Your message:

what is LoRA? explain how GRPO works in more depth, and how GEPA differs
Attach
Assistant Reply:

Here’s a crisp breakdown.

What is LoRA?

    Low-Rank Adaptation (LoRA) is a parameter-efficient finetuning technique. Instead of updating all model weights, LoRA injects small, trainable low-rank matrices into selected weight layers (e.g., attention projections q, k, v, o and MLP up/down/gate). During training, only these low-rank adapters are learned; the original backbone remains frozen.
    Benefits:
        Much fewer trainable parameters → cheaper, faster training and storage.
        Multiple task adapters can be swapped without duplicating the full model.
        Common in RL or supervised finetuning when full-parameter updates are too costly.

How GRPO works (deeper overview)

    Purpose: Group Relative Policy Optimization is an RL algorithm for LLMs with verifiable rewards. It learns to improve generation policies from scalar rewards (e.g., exact match, pass@k, rule satisfaction).

    Core ideas:
        Grouped rollouts per prompt/input:
            For each training instance, the policy samples a group (batch) of candidate outputs (e.g., 12 completions).
        Relative advantages inside the group:
            Instead of needing a separate value function, GRPO computes each sample’s advantage relative to others in its group—e.g., by subtracting a group baseline (mean or a reference statistic) so that “better-than-peers” generations get positive advantage and “worse-than-peers” get negative.
        Policy update (PPO-style constraint):
            With those advantages, it applies a clipped policy-gradient (akin to PPO) to increase the log-probability of higher-reward completions and decrease it for lower-reward ones, while constraining updates to avoid instability.
        Parameterization and efficiency:
            Often trained with LoRA adapters to keep training efficient (freeze backbone, update adapters).
        Rollout scale:
            Needs many environment interactions (rollouts) because each gradient step consumes groups of samples and rewards are sparse and scalar.

    Practical training loop (simplified):
        For each step: pick a small set of prompts, sample k generations per prompt, score them with a verifiable reward, compute group-relative advantages, backprop policy loss with clipping (plus entropy/regularization), periodically validate and early-stop.

What GEPA is and how it differs from GRPO

    Plane of learning:
        GRPO: updates model weights (policy parameters), typically via LoRA. Needs training access; model becomes specialized for the task.
        GEPA: updates prompts/instructions and module configurations in a compound AI system. No weight updates; works with black-box APIs.

    Learning signal:
        GRPO: scalar rewards per rollout (e.g., 0/1 EM, pass/fail). Gradient must infer what to change from sparse signals.
        GEPA: uses rich textual feedback from execution traces and evaluation traces (μf), enabling natural-language reflection to pinpoint failure causes and propose targeted instruction edits.

    Search/optimization strategy:
        GRPO: gradient-based RL; improves expected reward by stepping in parameter space.
        GEPA: evolutionary prompt optimization with:
            Reflective Prompt Mutation: LLM reads traces + feedback, writes improved instructions for a specific module.
            Pareto Frontier Candidate Selection: maintains all candidates that are best on at least one training instance (non-dominated); samples parents from this frontier, weighted by how often they appear, to preserve diverse “winning” strategies and avoid local optima.
            Optional System-Aware Merge: crossover that combines complementary modules from different lineages.

    Sample efficiency and budget:
        GRPO often needs tens to hundreds of thousands of rollouts in practice (even with LoRA) because it learns from sparse rewards and must explore a large action space.
        GEPA typically uses far fewer rollouts by extracting dense, language-level supervision from each attempt (diagnoses + concrete prompt edits), and by validating a compact set of evolving candidates.

    Scope and deployment:
        GRPO yields a new, tuned model checkpoint (adapters + base), affecting any downstream use of that checkpoint.
        GEPA yields an optimized set of prompts and module instructions for a particular compound system; easy to apply to any compatible model, including proprietary ones, without retraining.

    When to use which:
        Use GRPO (or other RL finetuning) when you control training, have large budgets, and want globally altered model behavior.
        Use GEPA when you need task adaptation without touching weights, want to stay API-only/black-box, or must be highly sample- and cost-efficient. It’s especially good for multi-module agent/program setups where module-level feedback exists.

Quick analogy

    GRPO = teaching the model new habits by adjusting its “brain” (weights) from rewards.
    GEPA = teaching the system new procedures by revising its “playbook” (prompts), guided by concrete postmortems written in natural language, and keeping a portfolio of best playbooks across tasks (Pareto frontier).

GPT-5
56.49 tok/sec
1065 tokens
Time-to-First: 4.5 sec
Your message:

How does GEPA compare to other prompt compilation techniques, such as DSPy, LMQL or the recent MTP? Can it be used in concert with their outputs/their existing RL passes
Attach
Assistant Reply:

Short answer

    GEPA is complementary to DSPy/LMQL/MTP. It focuses on reflective prompt evolution for compound systems, using textual feedback and Pareto-front sampling. You can use GEPA on top of, or in concert with, those frameworks’ programs and even after their RL/finetuning passes.

Comparison at a glance

    GEPA (this paper)
        What it optimizes: Instructions/prompts of each module in a multi-module (compound) system.
        How: Iterative reflective mutation (using execution/evaluation traces as rich text feedback), Pareto-front candidate selection, optional system-aware crossover (merge).
        Requirements: Black-box compatible (no weight updates). Uses rollouts plus any available textual feedback.
        Strengths: High sample efficiency; preserves diverse “winning” strategies; works with proprietary models; short, instruction-centric prompts.

    DSPy (Demonstrate–Search–Predict + optimizer suite)
        What it is: A programming model for LLM pipelines with pluggable optimizers (e.g., MIPROv2, GRPO). Emphasizes compiling declarative specs into prompts/few-shots and retrieval wiring.
        Typical optimization: Few-shot bootstrapping and instruction tuning via Bayesian search (e.g., MIPROv2), optionally RL variants (e.g., GRPO).
        GEPA interplay: GEPA can optimize the prompts inside DSPy programs. You can run GEPA as an alternative optimizer over the same modules, or after a DSPy/MIPROv2 pass to further refine instructions using reflection and Pareto sampling.

    LMQL (Language Model Query Language)
        What it is: A declarative language to script model calls with control flow, constraints, and scoring in a single program (compiled to model interactions).
        Typical use: Express constraints (beam/branching, token-level filters), retrieval, and tool use; less about automated prompt evolution.
        GEPA interplay: Treat the LMQL program as the compound system Φ. GEPA can mutate the system prompts embedded in LMQL modules while leaving LMQL control logic intact. Evaluation traces gathered via LMQL runtime can feed GEPA’s reflection.

    MTP (Multi-Task/Module “prompt compilation” techniques; likely the recent Multi-Template or Multi-Tool/Task Prompting works you’re referring to)
        Common pattern: Pre-compile prompts/templates for families of tasks or tools, sometimes with RL or search to pick/generate templates.
        GEPA interplay: Use MTP to produce strong seed prompts/templates across tasks; then run GEPA to specialize and evolve them per module using task-specific feedback, or to adapt them to a new dataset/domain with few rollouts.

Using GEPA together with RL or prior optimizers

    After RL (e.g., GRPO/LoRA): You can keep the tuned checkpoint and then run GEPA to optimize prompts around it. GEPA exploits rich textual signals (compiler errors, constraint failures) that RL may not fully capture, yielding additive gains.
    Before RL: Run GEPA first to craft clearer instructions and scaffolding, then do RL on weights. Cleaner prompts can reduce exploration burden and improve reward density.
    With Bayesian prompt search (e.g., MIPROv2 in DSPy): Use MIPROv2 to co-optimize few-shots and instructions, then let GEPA do reflective fine edits and Pareto-front diversification, often shrinking prompts while boosting generalization.
    With program frameworks (DSPy/LMQL): Keep the program (control flow, tools) and let GEPA mutate only module instructions. GEPA’s “system-aware merge” can combine different lineages’ best module prompts within the same program.

When to choose what

    If you need black-box compatibility, low rollout budgets, or you rely on rich textual evaluators (unit tests, retrievers, policy checkers), GEPA shines.
    If you can train weights and want broad behavior change, run RL (GRPO, etc.)—then layer GEPA on top to harvest additional gains from textual feedback.
    If your framework already expresses the pipeline (DSPy/LMQL), GEPA slots in as an optimizer for the prompts those modules use.

Practical recipe

    Start with your existing pipeline (DSPy or LMQL or a custom agent).
    Seed module prompts via your current method (handwritten, MIPROv2, MTP).
    Run GEPA with:
        μ: main evaluation metric (EM/F1, pass rate, rule satisfaction).
        μf: feedback function returning textual traces (e.g., constraint violations, missing docs, compiler errors).
        Pareto set over validation instances to guide which candidate to mutate next.
    Optionally, after GEPA converges, run a light RL pass (e.g., GRPO with LoRA) to adapt weights to the evolved instructions.
    Keep the best-performing candidate (prompts) as the compiled program for deployment.

GPT-5
64.87 tok/sec
1034 tokens
Time-to-First: 3.8 sec
Your message:

what are the most important observations from the experiments in the paper
Attach
Assistant Reply:

Here are the main takeaways the paper reports from its experiments:

    Reflective prompt evolution can beat RL on weights with far fewer rollouts
        GEPA outperforms GRPO (trained with 24,000 rollouts and LoRA) by up to 19% across tasks, while using up to 35× fewer rollouts to reach its best test scores.
        GEPA matches GRPO’s best validation scores after as few as 32–1,179 rollouts depending on the task, showing large sample-efficiency gains.

    GEPA consistently outperforms a leading prompt optimizer (MIPROv2)
        Across Qwen3‑8B and GPT‑4.1 mini on HotpotQA, IFBench, HoVer, and PUPA, GEPA (and GEPA+Merge) delivers larger aggregate gains than MIPROv2, often by 10–16 percentage points over baseline vs ~7 for MIPROv2.

    Pareto-based candidate selection is crucial
        Always mutating the single current “best” candidate tends to get stuck in local optima.
        Maintaining a Pareto frontier of instance-wise winners and sampling from it yields better final performance (up to +8.17% vs SelectBestCandidate) and more balanced search trees.

    Instruction optimization now generalizes well and is cheaper than few-shot prompts
        GEPA’s improved instructions often generalize better than few-shot–heavy prompts and are much shorter (up to ~9× shorter than MIPROv2 across some settings), cutting runtime token costs and latency while improving scores.

    System-aware crossover (Merge) can add gains, but timing/budget matter
        Combining module prompts from different strong lineages (merge) can give an extra ~2–5% in some settings (notably with GPT‑4.1 mini).
        Benefits depend on when merge is invoked and the search budget split between mutation and crossover.

    GEPA can act as an inference-time search strategy for code
        Preliminary results with GPT‑4o on NPUEval (AMD XDNA2 kernels) and KernelBench (CUDA for NVIDIA V100) show meaningful speed/quality improvements by evolving prompts using evaluator feedback (e.g., compiler errors), even without RAG or stochastic sampling, suggesting broader applicability to code optimization.

GPT-5
12.49 tok/sec
489 tokens
Time-to-First: 6.4 sec
Your message:

were there any problems gepa was better or worse for? how long does the compilation take in the shown examples, and how does that scale with prompt length/
Attach
Assistant Reply:

Here’s what the paper shows about where GEPA shines or struggles, plus timing/scale notes.

Where GEPA was better or worse

    Strong overall across tasks and models:
        On all four benchmarks (HotpotQA, IFBench, HoVer, PUPA), GEPA beat both GRPO (24k rollouts with LoRA) and MIPROv2 on aggregate, for both Qwen3‑8B and GPT‑4.1 mini.
    Cases where variant choice mattered:
        GEPA+Merge (the system-aware crossover) helped notably with GPT‑4.1 mini (adding roughly 2–5% in some settings), but could hurt on Qwen3‑8B depending on when merge was invoked and how the budget was split between mutation and crossover. The authors attribute this to suboptimal hyperparameters for Qwen3‑8B and the need to trigger merge only after sufficiently diverse lineages exist.
    Module diversity dependence:
        GEPA benefits most when the compound system has multiple modules and the evaluator can expose module- or hop-level feedback (e.g., missing docs at a hop, constraint failures) that the reflection step can exploit. Rich, targeted feedback makes GEPA’s mutations far more effective than scalar-only rewards.
    Few-shot heavy baselines:
        GEPA’s instruction-only prompts often outperformed few-shot–heavy configurations (e.g., MIPROv2) while being much shorter. That said, the authors note GEPA currently does not optimize exemplars; tasks that truly need demonstrations might gain further from adding exemplar optimization later.

How long compilation takes (rollout budgets) and scaling

    Budgets used in the paper
        GEPA’s total rollouts were matched (per benchmark) to MIPROv2’s budget; despite that, GEPA reached higher scores and often needed far fewer training rollouts to match/beat GRPO’s validation performance.
        Illustrative best-test points (Qwen3‑8B):
            HotpotQA: ~6,438 total rollouts (GEPA) vs GRPO trained with 24,000 rollouts; GEPA beat GRPO by ~19%.
            IFBench: ~678 total rollouts (≈35× fewer than 24,000) to reach its optimal test score, surpassing GRPO by ~2.7%.
            HoVer: ~6,858 total rollouts, +13.7% over GRPO.
            PUPA: ~2,157 total rollouts (≈11× fewer), +5.2% over GRPO.
        Train vs validation breakdown:
            Most of GEPA’s “counted” budget was spent on candidate validation (tracking the Pareto set), not on generating learning signals. If you count only training rollouts actually used for reflection/mutation, the numbers are very small (e.g., Hotpot ~737 train rollouts to optimal; only 102 to match GRPO’s best validation score).
    Wall-clock considerations
        Wall-clock depends on:
            LLM latency and cost (per module call in the system, per rollout).
            Evaluator runtime (e.g., retrieval, code compilation/tests).
            Number of candidates tracked on the Pareto set (each candidate gets validated).
        Because GEPA focuses on minibatch mutations (small b, e.g., 3) and validates on a fixed Pareto set, you can trade off runtime by shrinking the validation set or using dynamic/subsampled validation—authors note this as future work to further improve time/compute.
    Scaling with prompt length
        Inference cost scales with prompt token length, but GEPA’s evolved instructions are often much shorter than few-shot prompts:
            Up to about 9× shorter than MIPROv2 in some settings, and roughly 33% shorter on aggregate, while achieving higher scores.
        Shorter prompts reduce input-token cost and latency for every module call, which is significant in multi-module systems.
        GEPA’s optimization time itself is not dominated by prompt length; it’s dominated by the number of rollouts and validation evaluations. However, shorter prompts do reduce per-rollout latency/cost, so end-to-end compilation time and money improve when GEPA converges to compact instructions.

Practical guidance

    When GEPA is best:
        You have modular pipelines (retrieval, multi-hop, tool use) with available textual feedback.
        You need black-box compatibility and low sample budgets.
        You care about runtime efficiency of the final program (short prompts).
    When to be careful:
        If merge/crossover is used, tune when to invoke it and how much budget to allocate; invoke after diverse lineages exist.
        For tasks that depend critically on demonstrations, consider augmenting GEPA with exemplar optimization (future extension).

Bottom line

    GEPA was generally better across the evaluated tasks, with notable sample-efficiency and shorter prompts. It can be worse if crossover is mistimed or mis-budgeted (especially on some models), and its effectiveness grows with the availability and quality of module/evaluator textual feedback. Compilation time scales mainly with rollout count and validation set size; shorter evolved prompts help reduce per-rollout latency and final deployment costs.

GPT-5
34.71 tok/sec
1094 tokens
Time-to-First: 4.3 sec
