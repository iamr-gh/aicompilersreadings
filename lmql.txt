New Thread
Press Enter to send, Shift + Enter for new line
Attach
Your message:

This is a paper on a custom querying language for LLMs. Explain how the final language is destructured into LLM output, is it effectively a templatized prompt?
2212.06094v3.pdf
Attach
Assistant Reply:

Short answer: Partly yes—it’s templated, but with first-class variables, control flow, and constraints that are enforced during decoding.

How LMQL “destructures” into LLM output

    Prompt strings with holes: In LMQL, top-level strings can contain two special placeholders:
        [VAR]: a “hole” to be generated by the LLM. When the program reaches this, LMQL feeds the accumulated prompt to the model and decodes text to fill VAR.
        {var}: substitution of an already-bound variable back into the prompt (no decoding here).
    Execution model: The LMQL query is basically a Python-like script. As it runs:
        Plain strings get appended to the interaction trace.
        When it hits [VAR], LMQL invokes the decoder to generate just that segment, assigns it to VAR, and continues.
        When it hits {var}, LMQL inserts the previously generated value.
    Constraints during decoding: The where clause lets you write high-level constraints on hole variables (e.g., stops_at(EXPERT, "."), len(words(THING)) <= 2, VAR in some_list). LMQL compiles these into token masks on-the-fly, so the LLM is guided at each token step to only produce continuations that can still satisfy your constraints. This goes beyond a simple template because the constraints are eagerly enforced while decoding, not just validated after the fact.
    Control flow and tooling: You can use loops, branches, function calls, and even external tools. That means the “template” can be programmatic—e.g., iterate to produce multiple [ITEM] entries, choose decoding strategies (argmax, sample, scripted beam), and interleave tool outputs.
    Joint optimization across holes: Because decoding can consider multiple holes with control flow (especially with scripted beam search), LMQL can optimize choices jointly rather than fixing earlier text irrevocably before considering later parts.

So is it just a templatized prompt?

    It starts from a templating idea (strings with placeholders), but it’s more:
        Holes are generated segments with constraints enforced token-by-token.
        The where clause is declarative and compiled into decoding masks, not just post-hoc checks.
        You can add program logic (loops/ifs), external calls, and even distributions over the last hole (for classification).
        The runtime can reduce cost/latency by pruning invalid tokens early.

Mental model:

    Think “templated prompts with typed, constrained slots,” executed by a small program, where each slot is filled by an LLM under live constraints, and the program can branch/loop/integrate tools.

GPT-5
69.18 tok/sec
552 tokens
Time-to-First: 2.0 sec
Your message:

how are constraints enforced? is a token mask just a rejection of a stream of generation?
Attach
Assistant Reply:

Constraints are enforced eagerly during decoding, not just after the model finishes. LMQL compiles your high-level where constraints into token masks at each decoding step, so the model is only allowed to pick next tokens that can still lead to a valid completion.

How it works, step by step

    Eager validation: As the model generates a hole [VAR] token-by-token, LMQL partially evaluates your where expression on the current partial text. It tracks whether sub-conditions are already final (must be true/false), variable (could change), or monotone (can only increase/decrease).
    Follow semantics => token masks: For each possible next token t, LMQL uses “follow maps” to approximate whether choosing t can still yield a completion that satisfies the constraints. If a token is guaranteed to make the constraints unsatisfiable (final false), it’s masked out before sampling/argmax.
    Decoding with masks: The decoder computes softmax over the vocabulary, then elementwise multiplies by the mask (1 for allowed, 0 for disallowed), and renormalizes. If no tokens remain, decoding for that hole stops as invalid.
    Soundness over completeness: The masking aims to be sound (never disallow a token that could still lead to a valid completion). It may allow some tokens that later get rejected, but it tries to prune as much as it can safely.

Is a token mask just rejecting a stream after the fact?

    No—masks are applied before each token is chosen. This is different from generate-then-reject. It prevents the model from emitting invalid tokens in the first place, reducing backtracking and wasted tokens.
    If constraints can’t be fully enforced by masks (e.g., they depend on future content), LMQL may still have to backtrack or terminate a branch; but the mask prunes aggressively to minimize that.

Examples

    stops_at(VAR, "."): The follow map only allows tokens that keep a path to reaching a period boundary; once "." is produced, decoding for VAR ends.
    VAR in ["Stephen Hawking"]: If current text is "Steph", only next subtokens that keep a prefix of "en Hawking" stay unmasked; everything else is masked.
    len(words(VAR)) <= 3: The mask disallows tokens that would force a fourth word boundary; endings that keep the limit remain.

Why this matters

    Enforces constraints during generation (not post-hoc).
    Improves efficiency and cost: fewer invalid tokens generated, fewer API calls, less backtracking.
    Keeps a general interface: users write constraints over words/sentences/membership; LMQL translates them into token-level masks automatically.

GPT-5
56.67 tok/sec
552 tokens
Time-to-First: 2.5 sec
Your message:

how tightly is this integrated to a given LLM? is it possible to run such an approach without access to an open source LLM? or is the efficient token masks dependent on direct model access to turn things on and off?
Attach
Assistant Reply:

Short answer: You need access to the model’s next-token logits (or at least a way to apply per-token masks before sampling). With most closed APIs this isn’t fully available, so LMQL’s most efficient masking is easiest with open-source or locally hosted models, but partial variants are possible with some vendor APIs.

Details

    What LMQL needs to be fully efficient:
        Per-step next-token probabilities/logits.
        The ability to apply a binary mask over the vocabulary before the token is chosen, then renormalize.
        Tight loop control (stepwise decoding) so constraints can be re-evaluated after each token.

    How tightly coupled to a specific LLM?
        Not to a specific architecture: It treats the LLM as a black box that produces logits over a fixed vocab given a prefix.
        But it is tightly coupled to the decoding loop: the runtime must inject masks at each step (or at least before sampling/argmax). This is why it integrates cleanly with libraries like Hugging Face Transformers, where you can hook into generate().

    Can it work without an open-source model?
        Fully featured: Only if the API exposes logit_bias/logit_mask-like controls at the token level on every step. Few APIs provide this with sufficient granularity and low latency.
        Partial support: Some APIs (e.g., OpenAI) expose “logit bias” controls. You can emulate masks by heavily penalizing disallowed tokens. That enables a subset of constraints (e.g., label sets, stop tokens, light prefix restrictions), but more complex, dynamic constraints and fine-grained subtoken handling are harder.
        Post-hoc or chunked approaches: Without stepwise masking, you can generate in chunks, parse/validate, and retry/backtrack. This preserves correctness but loses most efficiency gains (more tokens wasted, more API calls).

    Why direct access matters
        The mask must be applied before sampling to avoid generating invalid tokens and paying for them. If you can only reject after the fact, you incur cost/latency and may need backtracking.
        Subword vocabularies require checking multiple valid prefix subtokens; implementing this well typically needs local tokenizer access and per-step control.

    Practical options
        Best: Run open-source models (HF Transformers or similar) locally or via a service that exposes stepwise logits and allows custom masks.
        Next best: Use vendor APIs that support per-token logit bias on each step; implement as much of the masking as allowed by the API.
        Fallback: Chunked decode + validate + retry. Works functionally, but you lose most efficiency benefits.

Bottom line

    The approach is model-agnostic but decoding-loop–dependent. You can use it with any model that lets you apply per-step token masks (or strong logit biases). For closed APIs that don’t expose this, you can approximate or run a less efficient variant; to get the full efficiency and tight constraint enforcement, you typically need open-source/local models or an API that offers fine-grained logit control every step.

GPT-5
59.00 tok/sec
644 tokens
Time-to-First: 1.8 sec
Your message:

how is context managed on the backtracks? furthermore, how does the system determine how far back it needs to go
Attach
Assistant Reply:

High-level idea

    LMQL tries to avoid backtracking by masking out “doomed” next tokens. When constraints can’t be guaranteed by masking alone (e.g., depend on future content), it may need to backtrack. How far it backtracks depends on the decoding mode and where the constraint became definitively unsatisfiable.

Two main cases

    Single-trajectory decoding (argmax/sample n=1)

    Eager constraint checking runs after every token. If at some step the mask becomes empty (no legal next tokens) or a constraint becomes final-false, the current hole cannot be completed legally.
    Recovery options:
        Abort that hole and return failure (common default in single-trajectory modes).
        Or, if you enable a search strategy (beam) or implement your own retry policy, you can back up to a prior decision point (see next).

    Search-based decoding (beam/scripted beam)

    LMQL keeps multiple hypotheses (beams), each with:
        Its own interaction trace (context/prefix)
        Current hole values
        Decoder state (e.g., KV cache)
        Validation state for constraints
    When a hypothesis becomes invalid (mask empty or constraint final-false), that beam is pruned immediately—no further expansion. There’s no “rewind” inside that beam; instead, search continues with the remaining viable beams that took different earlier choices. This is effectively backtracking “by branching”: earlier alternatives are already being explored in parallel.

How “how far back” is determined

    Constraint finality drives pruning: LMQL’s partial evaluation (final/var/inc/dec) and follow maps identify the earliest point at which an outcome is definitively impossible. The first time a constraint becomes final-false for a beam, that beam is pruned at that exact step.
    In beam search, the “distance back” is implicit in which alternative token choices at prior timesteps are still alive in other beams. You don’t roll the tape back; you discard the dead path and proceed with paths that diverged earlier.
    If you were to implement explicit backtracking without beams, you’d keep a stack of decision points (tokens where multiple options remained). When a constraint becomes impossible, you pop to the last decision point with untried options. LMQL’s paper frames this as the naive approach; LMQL prefers masking and beam-like branching to avoid expensive deep rewinds.

Context and caches

    Each hypothesis maintains its own context: the exact token prefix, hole bindings, and the model’s incremental state (KV cache for transformers). When a beam is pruned, its state is discarded. Surviving beams continue with their own caches—no need to recompute from scratch.
    Because masking is applied before token selection, many impossible paths are never expanded, keeping caches focused on viable contexts.

When backtracking is unavoidable

    Some constraints require lookahead across holes or long-range conditions that masking can’t fully capture soundly. In those cases:
        With beam search: earlier alternative beams serve as your backtrack. If all beams die, the query fails.
        Without beam search: you’d need an explicit backtracking policy (not the default), which replays from the last saved decision point and continues with the next allowable token.

Practical summary

    LMQL minimizes backtracking via:
        Eager partial evaluation to detect impossibility early.
        Follow-map–driven token masks to prevent invalid next tokens.
        Branch-and-prune (beam) instead of rewind-in-place.
    “How far back” is effectively “to the nearest surviving alternative,” which, in beam search, is already running in parallel. If no alternatives remain, decoding for that hole (or the whole query) fails.

GPT-5
61.62 tok/sec
779 tokens
Time-to-First: 1.9 sec
Your message:

Explain how beam search works
Assistant Reply:

Beam search is a decoding algorithm that explores multiple candidate sequences in parallel and keeps only the most promising ones at each step. It’s a middle ground between greedy decoding (one path) and exhaustive search (all paths).

Core idea

    Maintain a fixed-size set (beam) of partial sequences, each with a cumulative score (typically the sum of log-probabilities of its tokens).
    At each time step, expand each sequence by one token, score all expansions, and keep only the top-k overall as the next beam.
    Repeat until sequences end (e.g., emit EOS) or a length/stop condition is met. Finally, choose the best completed sequence (or N-best).

Step-by-step

    Initialize

    Start with an empty sequence (or the prompt context) with score 0 in the beam.
    Beam size k is a hyperparameter (e.g., 3, 5).

    Expand

    For each sequence in the beam, get the model’s next-token distribution given its prefix.
    Consider the top M tokens per sequence (often full vocab or a pruned set via top-k/top-p).
    Form new candidates by appending each token to each sequence; update the score by adding log P(token | prefix).

    Prune

    From all candidates across all current beam sequences, keep the top k by score; discard the rest.
    Optionally apply penalties/constraints (e.g., no-repeat n-gram, length normalization).

    Terminate

    Continue until:
        A sufficient number of sequences have emitted EOS, or
        A max length is reached, or
        All beams are finished/invalid.
    Return the best completed sequence(s). Some implementations keep generating until the best finished sequence is better than any unfinished one by a margin.

Why it helps

    Greedy decoding can miss globally better sequences because a slightly suboptimal early choice might lead to much better later tokens. Beam search keeps multiple hypotheses to hedge against early mistakes.

Scoring details

    Cumulative score: sum of log-probs is standard.
    Length normalization: without it, longer sequences tend to get lower scores (more terms). Common fixes:
        Divide by length^α (α > 0)
        Or add a length penalty term.
    Diversity: variants like diverse beam search add penalties to encourage beams to differ.

Practical variants and options

    Top-k/top-p pruning at each expansion to reduce computation.
    No-repeat n-gram constraint to prevent loops.
    Temperature can be applied to soften distributions, but classic beam search is deterministic.

In LMQL’s “scripted beam search”

    The same beam concept applies, but beams can branch not only on next tokens, but also across “holes” and control flow in the prompt program. Each beam carries:
        Its own interaction trace (prefix),
        Variable bindings for holes,
        Model state (KV cache),
        Validation state for constraints.
    Constraints can mask tokens per beam before scoring; beams that violate constraints are pruned immediately. This integrates constraint satisfaction into the search, improving efficiency and adherence to required formats.

GPT-5
75.93 tok/sec
647 tokens
Time-to-First: 1.9 sec
Your message:

so beam search is equivalent to some kind of weighted bfs?
Assistant Reply:

Pretty much, yes. Classic beam search is like a width-limited, weighted breadth-first search over the sequence tree.

    State space: nodes are partial sequences (prefixes).
    Edge weights: negative log-prob of the next token (so path cost is cumulative negative log-prob).
    Expansion: at each depth (time step), you expand all nodes in the current frontier (BFS-like).
    Pruning: instead of keeping the entire next frontier, you keep only the top-k by cumulative score (the “beam”).
    Termination: stop when EOS or max length; pick the best finished path.

So it’s BFS by levels with a global top-k cut at every layer using path scores. Not identical to standard uniform-cost/A* (no priority queue over depths), but the BFS analogy with per-level weighted pruning is a good mental model.
GPT-5
59.70 tok/sec
181 tokens
Time-to-First: 1.8 sec
