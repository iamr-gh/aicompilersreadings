New Thread
Press Enter to send, Shift + Enter for new line
Attach
Your message:

I'm reading the TVM paper(attached). I understand it is foundational for many subsequent ML compilers. They appear to be foundational in what came to be known as "auto scheduling", did they do any autotuning as well?
tvm.pdf
Attach
Assistant Reply:

Yes.
TVM already contains a full auto-tuning loop, not â€œonlyâ€ an auto-scheduler.

What the paper calls the â€œautomated program optimization frameworkâ€ (Â§ 5) is exactly an auto-tuner:

    A search space of legal schedules (loop order, tile sizes, unroll factors, tensorization choices, etc.) is built for every operator.
    An ML-based cost model (gradient-tree boosting in the paper) predicts the relative speed of each schedule.
    A search strategy (parallel simulated annealing) proposes candidates; the best ones are measured on real hardware.
    The measured run-times are fed back to re-train the cost model, closing the loop.

So TVM performs classic auto-tuning: generate many variants, measure a subset, and use the feedback to converge on the fastest implementation. The later term â€œauto-schedulingâ€ that you see in Ansor/TVM-Ansor papers is essentially an extension of this same idea with a larger, more systematic search space and a learned scheduler, but the seed is already present in the 2018 TVM paper.
Kimi K2 (0905)
55.15 tok/sec
234 tokens
Time-to-First: 5.8 sec
Your message:

One of their inspirations, Halide, ends up shipping an autoscheduler in 2019, and there was a subsequent one called tiramisu. In modern ml compilers, TVM never really became relevant, getting overshadowed by first party alternatives like XLA, torch inductor(especially torch 2). It seems like even Halide still has more industry traction. Why is this?
Attach
Assistant Reply:

TVM never disappeared; its ideas were absorbed and its brand was eclipsed by â€œfirst-partyâ€ stacks that could move faster and ship inside the frameworks users already call.
The paper you read laid the groundwork, but six practical realities kept TVM from becoming the everyday compiler most practitioners invoke:

    Ecosystem gravity
    â€“ PyTorch/TensorFlow ship their own â€œjust press compileâ€ buttons (TorchInductor, XLA, Torch-TensorRT, etc.).
    â€“ Users stay inside the framework; no extra package to build or tune.
    â€“ Framework owners can change IR, op definitions and runtime in lock-step, something an external project canâ€™t.

    Maintenance cost of a universal IR
    â€“ TVM tried to be a one-size-fits-all graph and tensor IR for every target. Keeping every front-end and back-end in sync became a combinatorial test matrix.
    â€“ First-party compilers only need to handle their own ops and a handful of targets (NVGPU, AMD, CPU, Apple, TPU).

    Search-time friction
    â€“ TVMâ€™s auto-tuner measured hundreds/thousands of kernels on real hardware; CI clusters rarely give you that luxury.
    â€“ TorchInductor/XLA rely on heuristics + light cost models + vendor libraries; compile time is seconds, not minutes.

    Library fallback strategy
    â€“ Inductor/XLA start with cuDNN/cuBLAS/oneDNN kernels and only synthesize when profitable.
    â€“ TVM historically generated everything from scratch; on â€œboringâ€ layers it was often slower than the vendor library.

    Engineering velocity
    â€“ Meta/Google/Apple dedicate teams that land patches in the same repo as the framework; no cross-company RFC process.
    â€“ Feature flags can be flipped for the next nightly wheel; TVM releases lagged months.

    Branding & documentation
    â€“ â€œpip install torch; torch.compile(model)â€ is one line.
    â€“ TVMâ€™s Python API changed across releases and the docs assumed you knew Halide-style scheduling. Casual users bounced.

Halide survives in industry because it never tried to be a universal DL framework; imaging/vision pipelines (Google Photos, Adobe, phone ISPs) are smaller, more specialized, and the schedule language is worth the learning curve. TVM aimed bigger and hit the classic â€œcross-framework IRâ€ adoption wall.

So TVMâ€™s research agenda (graph + op auto-tuning, tensorization, ML cost models, heterogeneous targets) is alive, but it lives under the hood of TorchInductor, Appleâ€™s MLIR stack, ARMâ€™s Ethos-U toolchain, Qualcommâ€™s SNPE, Huaweiâ€™s CANN, etc. The original TVM repo is still developed (OctoML, Apache) for vendors who need a custom back-end, but for the average PyTorch user itâ€™s now an invisible ancestor rather than the tool they type every day.
Kimi K2 (0905)
141.67 tok/sec
612 tokens
Time-to-First: 2.4 sec
Your message:

Does MLIR implement/replace most of the IR innovations pioneered in this paper? it feels like the ease of lowering from a graph -> operator -> low level implementation ends up being picked up by others
Attach
Assistant Reply:

MLIR did not so much â€œreplaceâ€ the TVM innovations as absorb and generalise them.
The key ideas that appear for the first time in the 2018 TVM paperâ€”graph-level + operator-level joint optimisation, declarative tensor expressions, schedule primitives that can be retargeted, and machine-learningâ€“guided searchâ€”all re-surface in MLIR, but in a more modular, dialect-based form.

What TVM did once inside a single IR (Relay â†’ TE â†’ TIR â†’ LLVM / CUDA / OpenCL) MLIR now does with a ladder of lowering dialects:

graph dialects
(TensorFlow, Torch, ONNX, TOSA, â€¦)
â†“
high-level structured op dialects
(linalg, tensor, mhlo, thlo, â€¦)
â†“
transformation dialects
(affine, scf, gpu, memref, amdgpu, nvvm, rocdl, â€¦)
â†“
target-specific dialects
(LLVM, SPIRV, PTX, VMVX, etc.)

Each rung is optional, so a vendor can insert the same kinds of optimisations TVM pioneeredâ€”operator fusion, tiling, tensorization, latency-hiding, explicit buffer scopingâ€”without re-inventing a monolithic IR.
In other words:

TVM 2018 invention â†’ MLIR 2020+ dialect
Relay graph â†’ TOSA / mhlo / torch-mlir
Tensor expression â†’ linalg.generic / linalg.matmul
Schedule primitive â†’ transform.loop.tile, transform.bufferize, etc.
Auto-tuning â†’ iree-compiler auto-tuner, XLAâ€™s autotuning framework, TorchInductorâ€™s search, etc.

So the ease of lowering you mention is now a commodity provided by MLIRâ€™s infrastructure; TVMâ€™s original stack was the existence proof that such a path could be made to work end-to-end.
Kimi K2 (0905)
149.68 tok/sec
401 tokens
Time-to-First: 3.1 sec
Your message:

How does OpenMP compare to some of the cpu scheduling optimizations they mention in this paper?
Attach
Assistant Reply:

OpenMP and TVMâ€™s CPU scheduling are solving different layers of the same problemâ€”parallelismâ€”but they operate at different abstraction levels and with different goals.
ğŸ” OpenMP: High-Level, User-Directed Parallelism

    What it is: A directive-based API (#pragma omp ...) for shared-memory parallelism.
    What you do: Add pragmas around loops or blocks; the compiler + runtime handle threading.
    What it controls:
        Thread creation/joining
        Work distribution (schedule(static/dynamic))
        Reductions, atomics, barriers
    What it doesnâ€™t do:
        Loop tiling, unrolling, vectorization, cache-aware layout, micro-kernel selection
        Auto-tuning tile sizes or prefetch distance
        Operator fusion across tensor ops

ğŸ”§ TVM CPU Scheduling: Compiler-Driven, Kernel-Level Optimization

    What it is: A compiler IR that generates the loop nest and explicitly schedules it for the target.
    What it controls (Â§4â€“5 of the paper):
        Loop tiling, unrolling, vectorization, thread binding
        Cache-level fusion (e.g., conv+relu in one tiled loop)
        Micro-kernel tensorization (e.g., 8Ã—8 GEMM using AVX512)
        Auto-tuning tile sizes, unroll factors, thread counts via ML cost model
    What it doesnâ€™t do:
        Manage thread lifetime or work-stealing (relies on OpenMP or pthreads for that)

ğŸ§ª Concrete Example: 1024Ã—1024 Matmul
Aspect	OpenMP	TVM
Parallelism	#pragma omp parallel for splits outer loop across threads	Compiler emits parallel(for) on tiled loops
Tiling	You must tile manually (or rely on compiler heuristics)	TVM searches over 4Ã—4, 8Ã—8, 32Ã—32 tile sizes
Vectorization	Hope the compiler auto-vectorizes; or use #pragma omp simd	Explicit vectorize primitive; knows ISA width
Fusion	Not applicable (single loop)	Fuses C = A*B; D = relu(C) into one tiled nest
Auto-tuning	None	Measures 200 candidates, picks fastest
ğŸ§© How They Overlap in Practice

Modern CPU back-ends inside TVM, XLA, TorchInductor, Halide, etc. emit OpenMP (or TBB/pthreads) for the outermost parallel loop, but only after they have:

    Tiled the loop to fit L2/L3
    Vectorized the innermost micro-kernel
    Fused multiple ops
    Auto-tuned tile sizes and unroll factors

So OpenMP becomes the runtime glue, while TVM (or any ML compiler) provides the performance-critical schedule.
ğŸ”‘ Take-away

OpenMP gives you portable threads;
TVM gives you portable performance on top of those threads.
They are complementary, not competitors.
Kimi K2 (0905)
53.27 tok/sec
747 tokens
Time-to-First: 7.7 sec
Your message:

Give a comprehensive list of all modern ml compiler chains inspired by tvm today
Attach
Assistant Reply:

Below is a â€œfamily treeâ€ that starts from the 2018 TVM paper and shows every production-grade or actively-researched ML compiler stack that inherited at least one of its core ideas (unified graph+operator IR, schedule primitives, ML-based auto-tuning, tensorization, latency-hiding, heterogeneous targets).
Items are grouped by the organisation that ships the stack to users; each bullet lists the key dialect / IR name and the public artifact you actually install.

Google / Alphabet

    XLA (HLO â†’ LHLO â†’ LMHLO â†’ LLVM / CUDA / ROCm / TPU)
    TensorFlow-TensorRT (TF-TRT) â€“ graph + operator fusion, auto-tuner inside
    JAX-XLA â€“ same HLO path, Python UX
    MLIR-HLO (mhlo) â€“ upstreamed dialect reused by IREE, StableHLO, etc.
    IREE (Flow â†’ Stream â†’ HAL â†’ LLVM / SPIR-V / CUDA / ROCm / WebGPU) â€“ explicit schedule-like â€œstreamâ€ abstraction, auto-tunes tile sizes

Meta / PyTorch

    TorchInductor (PrimTorch â†’ TorchDynamo â†’ TorchFX â†’ Triton / C++ / ATEN) â€“ uses integer-programming + auto-tuner for tile/vector sizes
    Glow (Glow IR â†’ LLVM / Habana / NNAPI) â€“ operator fusion + IR-level tiling
    Triton â€“ block-level IR, ML-cost-modelled tile-size search, backend emits LLVM-NVPTX / AMDGCN

Apple

    MLX (MLX IR â†’ LLVM / Metal) â€“ graph+kernel IR, fuses ops, schedules for M-series CPUs/GPUs
    CoreML compiler (NeuralNetwork â†’ MIL â†’ backend packages) â€“ layout + fusion passes inspired by TVM/Relay

Amazon / AWS

    Neo-AI (TVM fork) â€“ ships in SageMaker Neo, Panorama, Greengrass
    ONNX-Runtime-NNAPI / CoreML / XNNPACK â€“ optional TVM EP (execution provider)

Microsoft

    ONNX-Runtime-TVM EP â€“ same as above
    ONNX-ORT-ACL â€“ bridges to ARM-Compute, schedule space explored with TVM auto-tuner

Huawei

    CANN (Ascend) â€“ graph IR â†’ â€œTBEâ€ tensor engine, schedule language almost 1-to-1 clone of TVM TE, ships auto-tuner â€œAutoScheduleâ€

ARM

    Ethos-U / Ethos-N compiler â€“ uses â€œVelaâ€ backend, lowers to TVM-like IR, auto-tunes tile sizes

Intel

    Intel-Extension-for-PyTorch (IPEX) â€“ graph rewrite + kernel IR, auto-tunes tile/unroll for x86
    oneDNN Graph â€“ fuses ops, uses ML-based heuristic for cache-aware blocking

AMD

    ROCm-Comgr (MIOpen-Tensile) â€“ Tensile is YAML schedule spec + genetic search, direct descendant of TVM tensorization ideas
    XLA-AMD fork â€“ same HLO â†’ LLVM / ROCm path

Qualcomm

    Snapdragon-NPE (SNPE) â€“ converts ONNX/TF â†’ â€œDLCâ€, internal IR with TVM-style operator fusion + auto-tuned kernels
    QNN-Context â€“ same stack for Hexagon / HTA

Samsung

    ENN (Exynos-NN) â€“ TVM-derived IR, ships on Galaxy phones

NVIDIA

    TensorRT â€“ graph opt + kernel auto-tuner (Polyhedral + heuristics)
    cuDNN-backend â€œruntime fusion engineâ€ â€“ builds fused kernels on the fly, schedule space searched with ML model

Academia / Community

    Halide-Autoscheduler (2019) â€“ schedule language + beam-search, explicitly cites TVM
    Tiramisu â€“ polyhedral + schedule commands, auto-tuner uses integer programming
    Stripe-IR (Intel-Graph-Compiler) â€“ schedule tree, auto-tunes tile sizes
    Jax-Triton path â€“ same as above
    StableHLO â†’ IREE path â€“ community dialect, schedule search identical to TVM philosophy

Emerging (MLIR incubation)

    Torch-MLIR (Torch â†’ Linalg)
    TensorFlow-TOSA â†’ Linalg
    KernelGenerator (ARM) â€“ linalg-based, auto-tunes for Cortex-M/R

All of these stacks today:

    Keep a graph-level IR for fusion, layout, memory planning.
    Lower through an operator-level IR that exposes tiling, vectorization, unroll, parallelisation knobs.
    Use cost models + search (often gradient-boosting or RL) to pick knobs per kernel.
    Emit multiple backends (CPU, CUDA, ROCm, Metal, OpenCL, Vulkan, WebGPU, custom ASIC).

That combinationâ€”first demonstrated end-to-end by TVMâ€”has become the de-facto architecture for every modern ML compiler.
Kimi K2 (0905)
142.93 tok/sec
1056 tokens
Time-to-First: 5.1 sec
Your message:

What about IREE
Assistant Reply:

IREE is explicitly a child of the TVM lineageâ€”it just swapped out the â€œmonolithicâ€ Relay/TVM IR for MLIR dialects while keeping every core TVM idea (graphâ†’operatorâ†’lowering, schedule primitives, auto-tuning, multi-target).
In short: IREE is the modern, Google-led re-write of the TVM paperâ€™s vision.

Key points that map 1-to-1 to the 2018 paper
TVM paper concept	IREE realisation
High-level graph IR	stablehlo / tosa / torch dialects (same role as Relay)
Operator-level IR with schedule knobs	linalg, tensor, memref, scf, gpu dialects (TE + TIR replaced by composable MLIR)
Schedule primitives (tile, fuse, vectorize, tensorize)	transform dialect + linalg rewrite patterns
ML-based auto-tuner	IREEâ€™s â€œcompilation sandboxâ€ + evolutionary search + micro-kernel cost model
Multi-target back-ends	hal dialect â†’ LLVM-CPU, CUDA, ROCm, Metal, SPIR-V, WebGPU, VMVX
Latency-hiding / pipelining	stream dialect inserts async + barrier ops (same idea as TVMâ€™s virtual-thread lowering)
Deploy as static library / VM	IREE compiles to vmvx bytecode or AOT static library (TVMâ€™s â€œdeploy moduleâ€)

So IREE is not â€œanother alternativeâ€ to TVMâ€”it is the MLIR-era successor that keeps every design pattern the paper introduced, just expressed in reusable dialects instead of a single C++ IR.
Kimi K2 (0905)
50.96 tok/sec
376 tokens
Time-to-First: 5.5 sec
